{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [PrivateScore 0.66]KoBERT+RoBERTa+KoElectra+KR-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA를 통한 단어 전처리,\n",
    "\n",
    "'monologg/kobert',  # KoBERT 모델\n",
    "\n",
    " 'klue/roberta-base', # roberta 모델\n",
    "\n",
    " 'klue/roberta-small',\n",
    "\n",
    " 'klue/roberta-large',\n",
    "\n",
    " 'monologg/koelectra-base-v3-discriminator',  # koelectra 모델\n",
    "\n",
    " \"snunlp/KR-Medium\" # KR-BERT 모델\n",
    "\n",
    "위 모델들을 앙상블하여\n",
    "\n",
    "validation dataset에 대한 f1 score를 가중치로 한 Soft voting을 수행하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 모델들만 사용\n",
    "model_names = [\n",
    "    'klue/roberta-base',\n",
    "    'klue/roberta-small', \n",
    "    'klue/roberta-large',\n",
    "    'monologg/koelectra-base-v3-discriminator',\n",
    "    \"snunlp/KR-Medium\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Mode : False\n",
      "Train Mode : True\n"
     ]
    }
   ],
   "source": [
    "INFERENCE = False\n",
    "TRAIN = not INFERENCE\n",
    "print(f'Inference Mode : {INFERENCE}')\n",
    "print(f'Train Mode : {TRAIN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from eunjeon import Mecab\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_everything 함수 정의\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 시드 설정\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "config = {\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"epoch\": 5,\n",
    "    \"batch_size\": 64,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"max_len\": 128,\n",
    "    \"patience\":3,\n",
    "    \"lr_patience\":2\n",
    "}\n",
    "\n",
    "CFG = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Using cached konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting JPype1>=0.7.0 (from konlpy)\n",
      "  Using cached jpype1-1.5.2-cp312-cp312-macosx_10_9_universal2.whl.metadata (4.9 kB)\n",
      "Collecting lxml>=4.1.0 (from konlpy)\n",
      "  Downloading lxml-6.0.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from konlpy) (2.3.0)\n",
      "Requirement already satisfied: packaging in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Using cached konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "Using cached jpype1-1.5.2-cp312-cp312-macosx_10_9_universal2.whl (583 kB)\n",
      "Downloading lxml-6.0.0-cp312-cp312-macosx_10_13_universal2.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml, JPype1, konlpy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [konlpy]2m2/3\u001b[0m [konlpy]\n",
      "\u001b[1A\u001b[2KSuccessfully installed JPype1-1.5.2 konlpy-0.6.0 lxml-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Install MeCab in order to use it: http://konlpy.org/en/latest/install/",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/konlpy/tag/_mecab.py:77\u001b[39m, in \u001b[36mMecab.__init__\u001b[39m\u001b[34m(self, dicpath)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28mself\u001b[39m.tagger = \u001b[43mTagger\u001b[49m(\u001b[33m'\u001b[39m\u001b[33m-d \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m % dicpath)\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mself\u001b[39m.tagset = utils.read_json(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m/data/tagset/mecab.json\u001b[39m\u001b[33m'\u001b[39m % utils.installpath)\n",
      "\u001b[31mNameError\u001b[39m: name 'Tagger' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkonlpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Mecab\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m mecab = \u001b[43mMecab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(mecab.morphs(\u001b[33m\"\u001b[39m\u001b[33m한국어 형태소 분석을 시작합니다.\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/konlpy/tag/_mecab.py:82\u001b[39m, in \u001b[36mMecab.__init__\u001b[39m\u001b[34m(self, dicpath)\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mThe MeCab dictionary does not exist at \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m. Is the dictionary correctly installed?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mYou can also try entering the dictionary path when initializing the Mecab class: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMecab(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m/some/dic/path\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m % dicpath)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mInstall MeCab in order to use it: http://konlpy.org/en/latest/install/\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: Install MeCab in order to use it: http://konlpy.org/en/latest/install/"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "print(mecab.morphs(\"한국어 형태소 분석을 시작합니다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The MeCab dictionary does not exist at \"/Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages/eunjeon/data/mecabrc\". Is the dictionary correctly installed?\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab('/some/dic/path')\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/eunjeon/_mecab.py:101\u001b[39m, in \u001b[36mMecab.__init__\u001b[39m\u001b[34m(self, dicpath)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28mself\u001b[39m.tagger = \u001b[43mTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m--rcfile \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43mdicpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/eunjeon/mecab.py:528\u001b[39m, in \u001b[36mTagger.__init__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     this = \u001b[43m_MeCab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew_Tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/eunjeon/_mecab.py:104\u001b[39m, in \u001b[36mMecab.__init__\u001b[39m\u001b[34m(self, dicpath)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# Sometimes it works when we try twice.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28mself\u001b[39m.tagger = \u001b[43mTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m--rcfile \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43mdicpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/eunjeon/mecab.py:528\u001b[39m, in \u001b[36mTagger.__init__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     this = \u001b[43m_MeCab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew_Tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01meunjeon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Mecab\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m mecab = \u001b[43mMecab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(mecab.morphs(\u001b[33m\"\u001b[39m\u001b[33m한국어 형태소 분석을 시작합니다.\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/eunjeon/_mecab.py:106\u001b[39m, in \u001b[36mMecab.__init__\u001b[39m\u001b[34m(self, dicpath)\u001b[39m\n\u001b[32m    104\u001b[39m         \u001b[38;5;28mself\u001b[39m.tagger = Tagger(\u001b[33m'\u001b[39m\u001b[33m--rcfile \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m % dicpath)\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mThe MeCab dictionary does not exist at \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m. Is the dictionary correctly installed?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mYou can also try entering the dictionary path when initializing the Mecab class: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMecab(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m/some/dic/path\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m % dicpath)\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mInstall MeCab in order to use it: https://github.com/koshort/pyeunjeon/\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: The MeCab dictionary does not exist at \"/Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages/eunjeon/data/mecabrc\". Is the dictionary correctly installed?\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab('/some/dic/path')\""
     ]
    }
   ],
   "source": [
    "from eunjeon import Mecab\n",
    "mecab = Mecab()\n",
    "print(mecab.morphs(\"한국어 형태소 분석을 시작합니다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한국어', '형태소', '분석을', '시작합니다.']\n"
     ]
    }
   ],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "# 간단한 토큰화 예시\n",
    "text = \"한국어 형태소 분석을 시작합니다.\"\n",
    "words = text.split()  # 간단한 공백 기준 분리\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# '제목'과 '키워드' 결합\n",
    "train_df['제목_키워드'] = train_df['제목'] + train_df['키워드']\n",
    "test_df['제목_키워드'] = test_df['제목'] + test_df['키워드']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords.txt 파일이 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 기본 불용어 리스트\n",
    "stop_words = [\n",
    "    '이', '그', '저', '것', '수', '등', '및', '또는', '그리고', '하지만', '그런데',\n",
    "    '그러나', '따라서', '그래서', '왜냐하면', '만약', '만일', '아니면', '또한',\n",
    "    '이것', '저것', '그것', '무엇', '어떤', '어떻게', '언제', '어디서', '왜',\n",
    "    '가', '이', '을', '를', '의', '에', '에서', '로', '으로', '와', '과',\n",
    "    '도', '만', '부터', '까지'\n",
    "]\n",
    "\n",
    "# 파일로 저장\n",
    "with open(\"stopwords.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in stop_words:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(\"stopwords.txt 파일이 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 완료!\n",
      "훈련 데이터 샘플: 용인문화재단 인문학 콘서트 당신이 모르는 뮤지컬 이야기 개최 용인문화재단인문학콘서트뮤지컬이야기개최인문학콘서트뮤지컬이야기용인문화재단용인문화재단제공시민들심신여유안정인문학콘서트개최재단일큰어울마당용인시평생학습관인문학콘서트뮤지컬이야기인문학콘서트방송매체왕성활동뮤지컬평론가교수원종원순천향대진행이건명김소향김보경유건우배우들정상급뮤지컬배우라이브무대동시만끽공연공연홍지민뮤지컬배우참여라이브무대선사영화뮤지컬합성어무비컬주제맘마미아드림걸즈유명작품교수위트해설예정공연초등학생관람초등학생가능티켓가격전석공연용인문화재단누리집확인\n",
      "토큰 샘플: ['용인문화재단', '인문학', '콘서트', '당신이', '모르는', '뮤지컬', '이야기', '개최', '용인문화재단인문학콘서트뮤지컬이야기개최인문학콘서트뮤지컬이야기용인문화재단용인문화재단제공시민들심신여유안정인문학콘서트개최재단일큰어울마당용인시평생학습관인문학콘서트뮤지컬이야기인문학콘서트방송매체왕성활동뮤지컬평론가교수원종원순천향대진행이건명김소향김보경유건우배우들정상급뮤지컬배우라이브무대동시만끽공연공연홍지민뮤지컬배우참여라이브무대선사영화뮤지컬합성어무비컬주제맘마미아드림걸즈유명작품교수위트해설예정공연초등학생관람초등학생가능티켓가격전석공연용인문화재단누리집확인']\n",
      "레이블 개수: 56\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리에 필요한 불용어 리스트 작성 및 생성\n",
    "with open(\"stopwords.txt\",encoding = 'utf-8-sig') as f:\n",
    "    stop_words = f.readlines()\n",
    "stop_words = [word.strip() for word in stop_words]\n",
    "\n",
    "# 특수문자 제거 및 간단한 전처리 함수\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # 숫자 제거\n",
    "    text = re.sub(r'\\d+', '', str(text))\n",
    "    # 특수문자 제거 (한글, 영문, 공백만 남김)\n",
    "    text = re.sub(r'[^가-힣a-zA-Z\\s]', '', str(text))\n",
    "    # 연속된 공백을 하나로\n",
    "    text = re.sub(r'\\s+', ' ', str(text)).strip()\n",
    "    return text\n",
    "\n",
    "# 간단한 토큰화 함수 (형태소 분석 없이)\n",
    "def simple_tokenize(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    # 공백 기준으로 분리\n",
    "    tokens = str(text).split()\n",
    "    # 길이가 1보다 큰 토큰만 유지\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    return tokens\n",
    "\n",
    "# 데이터 전처리\n",
    "train_df['제목_키워드'] = train_df['제목'] + ' ' + train_df['키워드']\n",
    "test_df['제목_키워드'] = test_df['제목'] + ' ' + test_df['키워드']\n",
    "\n",
    "# 텍스트 정리\n",
    "train_df['제목_키워드'] = train_df['제목_키워드'].apply(clean_text)\n",
    "test_df['제목_키워드'] = test_df['제목_키워드'].apply(clean_text)\n",
    "\n",
    "# 간단한 토큰화 적용\n",
    "train_df['tokens'] = train_df['제목_키워드'].apply(simple_tokenize)\n",
    "test_df['tokens'] = test_df['제목_키워드'].apply(simple_tokenize)\n",
    "\n",
    "# 레이블 인코딩\n",
    "label_encoder = {label: i for i, label in enumerate(train_df['분류'].unique())}\n",
    "label_decoder = {i: label for label, i in label_encoder.items()}\n",
    "train_df['label'] = train_df['분류'].map(label_encoder)\n",
    "\n",
    "print(\"전처리 완료!\")\n",
    "print(f\"훈련 데이터 샘플: {train_df['제목_키워드'].iloc[0]}\")\n",
    "print(f\"토큰 샘플: {train_df['tokens'].iloc[0]}\")\n",
    "print(f\"레이블 개수: {len(label_encoder)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # jar 디렉토리가 없으면 생성\n",
    "    os.makedirs('./jar', exist_ok=True)\n",
    "    \n",
    "    # 레이블 인코더 저장\n",
    "    with open('./jar/label_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "\n",
    "    # 레이블 디코더 저장\n",
    "    with open('./jar/label_decoder.pkl', 'wb') as f:\n",
    "        pickle.dump(label_decoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INFERENCE:\n",
    "    # 레이블 인코더 불러오기\n",
    "    with open('./jar/label_encoder.pkl', 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "\n",
    "    # 레이블 디코더 불러오기\n",
    "    with open('./jar/label_decoder.pkl', 'rb') as f:\n",
    "        label_decoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/leeyoungho/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "# 동의어 찾기\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "# Synonym Replacement (동의어 교체)\n",
    "def synonym_replacement(words, n):\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set(new_words))\n",
    "    random.shuffle(random_word_list)\n",
    "    \n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:  # 교체된 단어 수 제한\n",
    "            break\n",
    "\n",
    "    return new_words\n",
    "\n",
    "# Random Insertion (무작위 삽입)\n",
    "def random_insertion(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "    return new_words\n",
    "\n",
    "def add_word(new_words):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    while len(synonyms) < 1:\n",
    "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "        if counter >= 10:  # 동의어를 찾을 수 없을 경우 중단\n",
    "            return\n",
    "    random_synonym = random.choice(synonyms)\n",
    "    random_idx = random.randint(0, len(new_words)-1)\n",
    "    new_words.insert(random_idx, random_synonym)\n",
    "\n",
    "# Random Swap (무작위 교환)\n",
    "def random_swap(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "    return new_words\n",
    "\n",
    "def swap_word(new_words):\n",
    "    if len(new_words) < 2:\n",
    "        return new_words\n",
    "    idx1, idx2 = random.sample(range(len(new_words)), 2)\n",
    "    new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
    "    return new_words\n",
    "\n",
    "# Random Deletion (무작위 삭제)\n",
    "def random_deletion(words, p):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:  # 확률적으로 삭제\n",
    "            new_words.append(word)\n",
    "\n",
    "    if len(new_words) == 0:  # 모든 단어가 삭제되면 한 단어를 남김\n",
    "        return [words[random.randint(0, len(words)-1)]]\n",
    "\n",
    "    return new_words\n",
    "\n",
    "# 증강 기법을 적용하여 여러 증강 텍스트 생성\n",
    "def augment_text(words, num_aug=4):\n",
    "    augmented_texts = []\n",
    "    \n",
    "    num_words = len(words)\n",
    "    n_sr = max(1, int(0.1 * num_words))  # Synonym Replacement 개수\n",
    "    n_ri = max(1, int(0.1 * num_words))  # Random Insertion 개수\n",
    "    n_rs = max(1, int(0.1 * num_words))  # Random Swap 개수\n",
    "    p_rd = 0.1  # Random Deletion 확률\n",
    "\n",
    "    augmented_texts.append(synonym_replacement(words, n_sr))  # 동의어 교체\n",
    "    augmented_texts.append(random_insertion(words, n_ri))  # 무작위 삽입\n",
    "    augmented_texts.append(random_swap(words, n_rs))  # 무작위 교환\n",
    "    augmented_texts.append(random_deletion(words, p_rd))  # 무작위 삭제\n",
    "    \n",
    "    return augmented_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 클래스의 데이터 수: label\n",
      "1     26950\n",
      "10     3454\n",
      "21     2568\n",
      "33     2318\n",
      "18     1480\n",
      "11      995\n",
      "7       966\n",
      "5       950\n",
      "20      845\n",
      "22      841\n",
      "25      711\n",
      "0       671\n",
      "36      640\n",
      "16      625\n",
      "14      621\n",
      "26      617\n",
      "3       608\n",
      "27      589\n",
      "28      537\n",
      "29      536\n",
      "15      447\n",
      "23      396\n",
      "9       387\n",
      "37      375\n",
      "6       349\n",
      "2       337\n",
      "31      335\n",
      "8       328\n",
      "4       327\n",
      "43      279\n",
      "44      248\n",
      "32      243\n",
      "13      238\n",
      "45      229\n",
      "17      221\n",
      "40      215\n",
      "19      213\n",
      "51      202\n",
      "42      190\n",
      "34      178\n",
      "35      173\n",
      "38      160\n",
      "12      128\n",
      "30      124\n",
      "47      113\n",
      "46      110\n",
      "48      102\n",
      "52       94\n",
      "54       90\n",
      "39       76\n",
      "49       67\n",
      "24       31\n",
      "55       29\n",
      "53       27\n",
      "50       17\n",
      "41        9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 각 클래스의 데이터 수 확인\n",
    "class_counts = train_df['label'].value_counts()\n",
    "print(\"각 클래스의 데이터 수:\", class_counts)\n",
    "\n",
    "# 최대 클래스 데이터 수 확인 (가장 많은 데이터가 있는 클래스)\n",
    "max_class_count = class_counts.max()\n",
    "\n",
    "# 2. 증강된 데이터를 저장할 리스트\n",
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "\n",
    "# 3. 각 클래스의 데이터 수를 최대값에 맞추기 위해 증강 적용\n",
    "for label in class_counts.index:\n",
    "    class_data = train_df[train_df['label'] == label]\n",
    "    current_class_count = class_counts[label]\n",
    "    \n",
    "    # 데이터 증강이 필요한 수만큼 반복\n",
    "    while current_class_count < max_class_count:\n",
    "        for tokens in class_data['tokens']:\n",
    "            # 증강된 데이터 생성\n",
    "            new_texts = augment_text(tokens)\n",
    "            \n",
    "            # 최대 클래스 수를 초과하지 않도록 조정\n",
    "            for new_text in new_texts:\n",
    "                augmented_texts.append(new_text)\n",
    "                augmented_labels.append(label)\n",
    "                current_class_count += 1\n",
    "                if current_class_count >= max_class_count:\n",
    "                    break\n",
    "            if current_class_count >= max_class_count:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "균형 맞춘 후 각 클래스의 데이터 수: label\n",
      "0     26950\n",
      "1     26950\n",
      "30    26950\n",
      "31    26950\n",
      "32    26950\n",
      "33    26950\n",
      "34    26950\n",
      "35    26950\n",
      "36    26950\n",
      "37    26950\n",
      "38    26950\n",
      "39    26950\n",
      "40    26950\n",
      "41    26950\n",
      "42    26950\n",
      "43    26950\n",
      "44    26950\n",
      "45    26950\n",
      "46    26950\n",
      "47    26950\n",
      "48    26950\n",
      "49    26950\n",
      "50    26950\n",
      "51    26950\n",
      "52    26950\n",
      "53    26950\n",
      "54    26950\n",
      "29    26950\n",
      "28    26950\n",
      "27    26950\n",
      "13    26950\n",
      "2     26950\n",
      "3     26950\n",
      "4     26950\n",
      "5     26950\n",
      "6     26950\n",
      "7     26950\n",
      "8     26950\n",
      "9     26950\n",
      "10    26950\n",
      "11    26950\n",
      "12    26950\n",
      "14    26950\n",
      "26    26950\n",
      "15    26950\n",
      "16    26950\n",
      "17    26950\n",
      "18    26950\n",
      "19    26950\n",
      "20    26950\n",
      "21    26950\n",
      "22    26950\n",
      "23    26950\n",
      "24    26950\n",
      "25    26950\n",
      "55    26950\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 4. 증강된 데이터프레임 생성\n",
    "augmented_df = pd.DataFrame({'tokens': augmented_texts, 'label': augmented_labels})\n",
    "\n",
    "# 5. 기존 데이터에 증강된 데이터 추가\n",
    "train_df = pd.concat([train_df, augmented_df], ignore_index=True)\n",
    "\n",
    "# 클래스별로 데이터 수 확인\n",
    "print(\"균형 맞춘 후 각 클래스의 데이터 수:\", train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 사용 안함!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터: 1207360, 검증 데이터: 301840\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할 (Word2Vec 없이)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=42)\n",
    "print(f\"훈련 데이터: {len(train_df)}, 검증 데이터: {len(val_df)}\")\n",
    "\n",
    "# 데이터셋 클래스 정의 (BERT 모델용)\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 데이터로더 생성 함수\n",
    "def create_data_loader(texts, labels, tokenizer, batch_size=16, max_len=128):\n",
    "    dataset = TextDataset(texts, labels, tokenizer, max_len)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터프레임들이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    # 데이터프레임을 피클로 저장\n",
    "    with open('./jar/train_df.pkl', 'wb') as f:\n",
    "        pickle.dump(train_df, f)\n",
    "    with open('./jar/test_df.pkl', 'wb') as f:\n",
    "        pickle.dump(test_df, f)\n",
    "    with open('./jar/val_df.pkl', 'wb') as f:\n",
    "        pickle.dump(val_df, f)\n",
    "    \n",
    "    print(\"데이터프레임들이 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INFERENCE:\n",
    "    # 데이터프레임 불러오기\n",
    "    with open('./jar/train_df.pkl', 'rb') as f:\n",
    "        train_df = pickle.load(f)\n",
    "    with open('./jar/test_df.pkl', 'rb') as f:\n",
    "        test_df = pickle.load(f)\n",
    "    with open('./jar/val_df.pkl', 'rb') as f:\n",
    "        val_df = pickle.load(f)\n",
    "    # 임베딩 매트릭스 불러오기\n",
    "    with open('./jar/embedding_matrix.pkl', 'rb') as f:\n",
    "        embedding_matrix = pickle.load(f)\n",
    "    # word2idx 사전 불러오기\n",
    "    with open('./jar/word2idx.pkl', 'rb') as f:\n",
    "        word2idx = pickle.load(f)\n",
    "    # idx2word 사전 불러오기\n",
    "    with open('./jar/idx2word.pkl', 'rb') as f:\n",
    "        idx2word = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer=None, word2idx=None, max_len=128, model_type='cnn'):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "        self.model_type = model_type  # 'cnn', 'lstm' or 'bert'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx] if self.labels is not None else -1\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                \" \".join(text),\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                return_token_type_ids=False,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        elif self.model_type in ['cnn', 'lstm']:\n",
    "            tokens = text\n",
    "            seq = [self.word2idx.get(token, self.word2idx[\"<UNK>\"]) for token in tokens]\n",
    "            if len(seq) < self.max_len:\n",
    "                seq += [self.word2idx[\"<PAD>\"]] * (self.max_len - len(seq))\n",
    "            else:\n",
    "                seq = seq[:self.max_len]\n",
    "            seq = torch.tensor(seq, dtype=torch.long)\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "            return {\n",
    "                'input_ids': seq,\n",
    "                'labels': label\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Choose 'bert', 'cnn', or 'lstm'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 모델들과 토크나이저 로드\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_names = [\n",
    "    'monologg/kobert',  # KoBERT 모델\n",
    "    'klue/roberta-base', # roberta 모델\n",
    "    'klue/roberta-small',\n",
    "    'klue/roberta-large',\n",
    "    'monologg/koelectra-base-v3-discriminator',  # koelectra 모델\n",
    "    \"snunlp/KR-Medium\" # KR-BERT 모델\n",
    "]\n",
    "\n",
    "models = {}\n",
    "tokenizers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967d22847d7f4059882194b695292fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/426 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5e70d526754fd99d55337c8c7fe928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/369M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cd1f423cc34d6eb3ad9943e00d02a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b337db3772fa4e8987f03cc644db704d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8291e98e2fd430f9b6ec815d9d1984b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b17866ae09a42ca9a1a58abead78c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6039b28c8cc489a9c8487415f221643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da306df51a7642a89913c95868bfb8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891ff710f91f45f791f43a09f66fad4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598e1191bed94d45b7ae482b7b598a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6939a230bc3405b9f5d471b6be17e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d52d336a554a08888bd8ea6f003dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed7234bea2f415e92b15d41c9aac333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/545 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408a980eda934db59e2494d17703077a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/273M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b69e6b2f2dc43e8a4d206a4f44fcfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1525a8a6e9074636a727c4f974bcb056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617909471fbf404a97d3b0f02f16cb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a592301e044389a81b9b83c6e9b9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226b1dfd174c46daba5c5d72dca85c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3986b78f9dc94de5bca5dc7649e9d269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0ec343e056470580e56f8910760f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/61.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3776079ade444f1ca8e240597bdf7064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/467 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d645ddcc493046dbaaa6bb2be4ef48ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a385c9f0ce0405980c5ac8ded1ad1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/452M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c347369eda53447daead0a17764b6433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ed61ac709d470ca522839f74c2270f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/337 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd8c0a196c340aca985117ee9209c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4422ecb14f4042839230d020608458d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/452M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f22ad1208d4cc099be63498241372c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/408M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-Medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_names:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(label_encoder),\n",
    "        trust_remote_code=True\n",
    "    ).to('mps')\n",
    "    tokenizers[model_name] = tokenizer\n",
    "    models[model_name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT 모델들의 데이터로더가 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# BERT 모델들만 사용\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loaders = {}\n",
    "val_loaders = {}\n",
    "test_loaders = {}\n",
    "\n",
    "# 배치 크기 설정 (모든 모델에 대해 동일하게)\n",
    "batch_size = CFG.batch_size\n",
    "\n",
    "for model_name in model_names:\n",
    "    tokenizer = tokenizers[model_name]\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    train_dataset = TextDataset(train_df['tokens'].tolist(), train_df['label'].tolist(), tokenizer=tokenizer, max_len=CFG.max_len, model_type='bert')\n",
    "    val_dataset = TextDataset(val_df['tokens'].tolist(), val_df['label'].tolist(), tokenizer=tokenizer, max_len=CFG.max_len, model_type='bert')\n",
    "    test_dataset = TextDataset(test_df['tokens'].tolist(), None, tokenizer=tokenizer, max_len=CFG.max_len, model_type='bert')\n",
    "\n",
    "    # 데이터로더 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_loaders[model_name] = train_loader\n",
    "    val_loaders[model_name] = val_loader\n",
    "    test_loaders[model_name] = test_loader\n",
    "\n",
    "print(\"BERT 모델들의 데이터로더가 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 훈련 데이터: 965888\n",
      "줄인 훈련 데이터: 10000\n",
      "원본 검증 데이터: 241472\n",
      "줄인 검증 데이터: 2000\n",
      "배치 크기: 64\n",
      "작은 데이터로 데이터로더가 다시 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 테스트용으로 데이터 크기 줄이기\n",
    "train_df_small = train_df.sample(n=10000, random_state=42)  # 1만개만 사용\n",
    "val_df_small = val_df.sample(n=2000, random_state=42)      # 2천개만 사용\n",
    "\n",
    "print(f\"원본 훈련 데이터: {len(train_df)}\")\n",
    "print(f\"줄인 훈련 데이터: {len(train_df_small)}\")\n",
    "print(f\"원본 검증 데이터: {len(val_df)}\")\n",
    "print(f\"줄인 검증 데이터: {len(val_df_small)}\")\n",
    "\n",
    "# 작은 데이터로 데이터로더 다시 생성\n",
    "train_loaders = {}\n",
    "val_loaders = {}\n",
    "test_loaders = {}\n",
    "\n",
    "batch_size = CFG.batch_size  # 배치 크기도 늘리기\n",
    "print(f\"배치 크기: {batch_size}\")\n",
    "\n",
    "for model_name in model_names:\n",
    "    tokenizer = tokenizers[model_name]\n",
    "    \n",
    "    # 작은 데이터셋으로 생성\n",
    "    train_dataset = TextDataset(train_df_small['tokens'].tolist(), train_df_small['label'].tolist(), tokenizer=tokenizer, max_len=CFG.max_len, model_type='bert')\n",
    "    val_dataset = TextDataset(val_df_small['tokens'].tolist(), val_df_small['label'].tolist(), tokenizer=tokenizer, max_len=CFG.max_len, model_type='bert')\n",
    "    test_dataset = TextDataset(test_df['tokens'].tolist(), None, tokenizer=tokenizer, max_len=CFG.max_len, model_type='bert')\n",
    "\n",
    "    # 데이터로더 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_loaders[model_name] = train_loader\n",
    "    val_loaders[model_name] = val_loader\n",
    "    test_loaders[model_name] = test_loader\n",
    "\n",
    "print(\"작은 데이터로 데이터로더가 다시 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: mps\n",
      "BERT 모델 5개의 옵티마이저가 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# BERT 모델들만 사용하므로 모델 초기화 단순화\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"사용 중인 디바이스: {device}\")\n",
    "\n",
    "# 손실 함수 정의 (BERT 모델들에 공통으로 사용)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# BERT 모델들의 옵티마이저들을 저장할 딕셔너리\n",
    "optimizers = {}\n",
    "\n",
    "# 각 BERT 모델에 대해 옵티마이저 생성\n",
    "for model_name in model_names:\n",
    "    optimizer = optim.AdamW(models[model_name].parameters(), lr=CFG.learning_rate)\n",
    "    optimizers[model_name] = optimizer\n",
    "\n",
    "print(f\"BERT 모델 {len(model_names)}개의 옵티마이저가 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수 정의 (BERT 모델들만 사용하므로 단순화)\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, model_name):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=CFG.lr_patience)\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0\n",
    "    print(f\"device: {device}\")\n",
    "    print(f\"=== {model_name} 모델 학습 시작 ===\")\n",
    "    for epoch in range(CFG.epoch):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Training loop\n",
    "        for batch in tqdm(train_loader, desc=f'{model_name} Epoch {epoch + 1}/{CFG.epoch}'):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"{model_name} - Epoch {epoch + 1} Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        val_total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                val_total_loss += loss.item()\n",
    "\n",
    "                _, preds = torch.max(outputs.logits, dim=1)\n",
    "                val_predictions.extend(preds.cpu().tolist())\n",
    "                val_true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "        avg_val_loss = val_total_loss / len(val_loader)\n",
    "        print(f\"{model_name} - Epoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # 검증 결과 출력\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='macro')\n",
    "        print(f\"{model_name} - Epoch {epoch + 1} Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "        # Learning Rate Scheduler 업데이트\n",
    "        scheduler.step(val_f1)\n",
    "        \n",
    "        # Early Stopping Check\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            # models 디렉토리 생성 후 모델 저장\n",
    "            os.makedirs('./models', exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'./models/best_{model_name.replace(\"/\", \"_\")}_model.pt')\n",
    "            print(\"best model saved\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= CFG.patience:\n",
    "                print(f\"{model_name} Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    return best_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INFERENCE:\n",
    "    with open('./jar/val_f1_scores.pkl','rb') as f:\n",
    "        val_f1_scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    val_f1_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(val_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== klue/roberta-base 모델 학습 시작 ===\n",
      "device: mps\n",
      "=== klue/roberta-base 모델 학습 시작 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "klue/roberta-base Epoch 1/5: 100%|██████████| 157/157 [10:00<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klue/roberta-base - Epoch 1 Training Loss: 1.3134\n",
      "klue/roberta-base - Epoch 1 Validation Loss: 1.3050\n",
      "klue/roberta-base - Epoch 1 Validation F1 Score: 0.6445\n",
      "best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "klue/roberta-base Epoch 2/5: 100%|██████████| 157/157 [10:41<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klue/roberta-base - Epoch 2 Training Loss: 1.0007\n",
      "klue/roberta-base - Epoch 2 Validation Loss: 1.1874\n",
      "klue/roberta-base - Epoch 2 Validation F1 Score: 0.6914\n",
      "best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "klue/roberta-base Epoch 3/5: 100%|██████████| 157/157 [10:20<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klue/roberta-base - Epoch 3 Training Loss: 0.7721\n",
      "klue/roberta-base - Epoch 3 Validation Loss: 1.1090\n",
      "klue/roberta-base - Epoch 3 Validation F1 Score: 0.7146\n",
      "best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "klue/roberta-base Epoch 4/5: 100%|██████████| 157/157 [11:15<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klue/roberta-base - Epoch 4 Training Loss: 0.5879\n",
      "klue/roberta-base - Epoch 4 Validation Loss: 1.0682\n",
      "klue/roberta-base - Epoch 4 Validation F1 Score: 0.7356\n",
      "best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "klue/roberta-base Epoch 5/5: 100%|██████████| 157/157 [08:31<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klue/roberta-base - Epoch 5 Training Loss: 0.4379\n",
      "klue/roberta-base - Epoch 5 Validation Loss: 1.0455\n",
      "klue/roberta-base - Epoch 5 Validation F1 Score: 0.7533\n",
      "best model saved\n",
      "klue/roberta-base 최고 F1 점수: 0.7533\n",
      "\n",
      "=== klue/roberta-small 모델 학습 시작 ===\n",
      "device: mps\n",
      "=== klue/roberta-small 모델 학습 시작 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "klue/roberta-small Epoch 1/5: 100%|██████████| 157/157 [05:37<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klue/roberta-small - Epoch 1 Training Loss: 3.4725\n",
      "klue/roberta-small - Epoch 1 Validation Loss: 2.7560\n",
      "klue/roberta-small - Epoch 1 Validation F1 Score: 0.3892\n",
      "best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "klue/roberta-small Epoch 2/5: 100%|██████████| 157/157 [07:02<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klue/roberta-small - Epoch 2 Training Loss: 2.4304\n",
      "klue/roberta-small - Epoch 2 Validation Loss: 2.0724\n",
      "klue/roberta-small - Epoch 2 Validation F1 Score: 0.4719\n",
      "best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "klue/roberta-small Epoch 3/5: 100%|██████████| 157/157 [06:28<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klue/roberta-small - Epoch 3 Training Loss: 1.7923\n",
      "klue/roberta-small - Epoch 3 Validation Loss: 1.6592\n",
      "klue/roberta-small - Epoch 3 Validation F1 Score: 0.5706\n",
      "best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "klue/roberta-small Epoch 4/5: 100%|██████████| 157/157 [06:11<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klue/roberta-small - Epoch 4 Training Loss: 1.3588\n",
      "klue/roberta-small - Epoch 4 Validation Loss: 1.4247\n",
      "klue/roberta-small - Epoch 4 Validation F1 Score: 0.6389\n",
      "best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "klue/roberta-small Epoch 5/5: 100%|██████████| 157/157 [06:33<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klue/roberta-small - Epoch 5 Training Loss: 1.0253\n",
      "klue/roberta-small - Epoch 5 Validation Loss: 1.2638\n",
      "klue/roberta-small - Epoch 5 Validation F1 Score: 0.6664\n",
      "best model saved\n",
      "klue/roberta-small 최고 F1 점수: 0.6664\n",
      "\n",
      "=== klue/roberta-large 모델 학습 시작 ===\n",
      "device: mps\n",
      "=== klue/roberta-large 모델 학습 시작 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "klue/roberta-large Epoch 1/5:   7%|▋         | 11/157 [31:49<7:02:23, 173.59s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m val_loader = val_loaders[model_name]\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m best_f1 = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m val_f1_scores[model_name] = best_f1\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 최고 F1 점수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(model, train_loader, val_loader, model_name)\u001b[39m\n\u001b[32m     26\u001b[39m     loss = outputs.loss\n\u001b[32m     28\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     total_loss += loss.item()\n\u001b[32m     33\u001b[39m avg_train_loss = total_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/torch/optim/adam.py:439\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    436\u001b[39m     device_beta1 = beta1\n\u001b[32m    438\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m \u001b[43mexp_avg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_beta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[38;5;66;03m# Nested if is necessary to bypass jitscript rules\u001b[39;00m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m differentiable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(beta2, Tensor):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 모델 학습 실행\n",
    "val_f1_scores = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n=== {model_name} 모델 학습 시작 ===\")\n",
    "    \n",
    "    model = models[model_name]\n",
    "    train_loader = train_loaders[model_name]\n",
    "    val_loader = val_loaders[model_name]\n",
    "    \n",
    "    # 모델 학습\n",
    "    best_f1 = train_and_evaluate(model, train_loader, val_loader, model_name)\n",
    "    val_f1_scores[model_name] = best_f1\n",
    "    \n",
    "    print(f\"{model_name} 최고 F1 점수: {best_f1:.4f}\")\n",
    "\n",
    "print(\"\\n=== 모든 모델 학습 완료 ===\")\n",
    "print(\"각 모델의 F1 점수:\")\n",
    "for model_name, f1_score in val_f1_scores.items():\n",
    "    print(f\"{model_name}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== klue/roberta-base 모델 학습 시작 ===\n",
      "device: mps\n",
      "=== klue/roberta-base 모델 학습 시작 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "klue/roberta-base Epoch 1/5:   0%|          | 0/157 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m train_loader = train_loaders[model_name]\n\u001b[32m     10\u001b[39m val_loader = val_loaders[model_name]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m best_val_f1 = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m val_f1_scores[model_name] = best_val_f1\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m model, model_path, train_loader, val_loader\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(model, train_loader, val_loader, model_name)\u001b[39m\n\u001b[32m     22\u001b[39m attention_mask = batch[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     23\u001b[39m labels = batch[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m loss = outputs.loss\n\u001b[32m     28\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:1191\u001b[39m, in \u001b[36mRobertaForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1174\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1175\u001b[39m \u001b[33;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1176\u001b[39m \u001b[33;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1187\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1188\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1189\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1203\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:824\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    817\u001b[39m         extended_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n\u001b[32m    818\u001b[39m             attention_mask,\n\u001b[32m    819\u001b[39m             input_shape,\n\u001b[32m    820\u001b[39m             embedding_output,\n\u001b[32m    821\u001b[39m             past_key_values_length,\n\u001b[32m    822\u001b[39m         )\n\u001b[32m    823\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m         extended_attention_mask = \u001b[43m_prepare_4d_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_length\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[32m    829\u001b[39m     \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[32m    830\u001b[39m     extended_attention_mask = \u001b[38;5;28mself\u001b[39m.get_extended_attention_mask(attention_mask, input_shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:457\u001b[39m, in \u001b[36m_prepare_4d_attention_mask_for_sdpa\u001b[39m\u001b[34m(mask, dtype, tgt_len)\u001b[39m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAttentionMaskConverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_expand_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:186\u001b[39m, in \u001b[36mAttentionMaskConverter._expand_mask\u001b[39m\u001b[34m(mask, dtype, tgt_len)\u001b[39m\n\u001b[32m    182\u001b[39m         mask.masked_fill_(context_mask, torch.finfo(dtype).min)\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mask[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :, :].expand(bsz, \u001b[32m1\u001b[39m, tgt_len, tgt_len + past_key_values_length)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_expand_mask\u001b[39m(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    188\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03m    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    191\u001b[39m     bsz, src_len = mask.size()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 다른 모델들 학습\n",
    "if TRAIN:\n",
    "    for model_name in model_names:\n",
    "        print(f\"=== {model_name} 모델 학습 시작 ===\")\n",
    "        model = models[model_name]\n",
    "        model_path = f'./models/best_{model_name.replace(\"/\", \"_\")}_model.pt'\n",
    "        if os.path.exists(model_path):\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        train_loader = train_loaders[model_name]\n",
    "        val_loader = val_loaders[model_name]\n",
    "        best_val_f1 = train_and_evaluate(model, train_loader, val_loader, model_name)\n",
    "        val_f1_scores[model_name] = best_val_f1\n",
    "        del model, model_path, train_loader, val_loader\n",
    "        # macOS에서는 torch.cuda.empty_cache() 대신 gc.collect()만 사용\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 세트 추론 (더 간단한 버전)\n",
    "test_probs = {}\n",
    "\n",
    "def predict_test(model, test_loader, model_name):\n",
    "    model.eval()\n",
    "    probs_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f'{model_name} Testing'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            probs = F.softmax(outputs.logits, dim=1)\n",
    "            probs_list.append(probs.cpu())\n",
    "\n",
    "    test_probs[model_name] = torch.cat(probs_list, dim=0)\n",
    "    print(f\"{model_name} 예측 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== klue/roberta-base 테스트 세트 추론 시작 ===\n",
      "모델 로드 완료: ./models/best_klue_roberta-base_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "klue/roberta-base Testing: 100%|██████████| 366/366 [02:51<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klue/roberta-base 예측 완료\n",
      "=== klue/roberta-small 테스트 세트 추론 시작 ===\n",
      "모델 로드 완료: ./models/best_klue_roberta-small_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "klue/roberta-small Testing: 100%|██████████| 366/366 [01:29<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klue/roberta-small 예측 완료\n",
      "=== klue/roberta-large 테스트 세트 추론 시작 ===\n",
      "모델 파일이 없습니다: ./models/best_klue_roberta-large_model.pt\n",
      "=== monologg/koelectra-base-v3-discriminator 테스트 세트 추론 시작 ===\n",
      "모델 파일이 없습니다: ./models/best_monologg_koelectra-base-v3-discriminator_model.pt\n",
      "=== snunlp/KR-Medium 테스트 세트 추론 시작 ===\n",
      "모델 파일이 없습니다: ./models/best_snunlp_KR-Medium_model.pt\n"
     ]
    }
   ],
   "source": [
    "# 다른 모델들 테스트 추론 (안전한 버전)\n",
    "for model_name in model_names:\n",
    "    print(f\"=== {model_name} 테스트 세트 추론 시작 ===\")\n",
    "    model = models[model_name]\n",
    "    test_loader = test_loaders[model_name]\n",
    "    \n",
    "    # 저장된 최적의 모델 로드 (안전하게)\n",
    "    model_path = f'./models/best_{model_name.replace(\"/\", \"_\")}_model.pt'\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f\"모델 로드 완료: {model_path}\")\n",
    "    else:\n",
    "        print(f\"모델 파일이 없습니다: {model_path}\")\n",
    "        continue\n",
    "    \n",
    "    predict_test(model, test_loader, model_name)\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'klue/roberta-base': 0.7533415056626291, 'klue/roberta-small': 0.6664335882279931}\n"
     ]
    }
   ],
   "source": [
    "print(val_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델별 가중치:\n",
      "klue/roberta-base: 0.5306\n",
      "klue/roberta-small: 0.4694\n",
      "앙상블 예측 완료: 23405개 샘플\n"
     ]
    }
   ],
   "source": [
    "# 앙상블 적용\n",
    "# 가중치 계산 (검증 F1 점수 기반)\n",
    "if val_f1_scores:  # val_f1_scores가 비어있지 않은지 확인\n",
    "    total_f1 = sum(val_f1_scores.values())\n",
    "    model_weights = {model_name: val_f1_scores[model_name] / total_f1 for model_name in val_f1_scores.keys()}\n",
    "    \n",
    "    print(\"모델별 가중치:\")\n",
    "    for model_name, weight in model_weights.items():\n",
    "        print(f\"{model_name}: {weight:.4f}\")\n",
    "    \n",
    "    # 앙상블 확률 계산\n",
    "    ensemble_probs = None\n",
    "    \n",
    "    for model_name, probs in test_probs.items():\n",
    "        if model_name in model_weights:  # 가중치가 있는 모델만 사용\n",
    "            weight = model_weights[model_name]\n",
    "            if ensemble_probs is None:\n",
    "                ensemble_probs = weight * probs\n",
    "            else:\n",
    "                ensemble_probs += weight * probs\n",
    "    \n",
    "    # 최종 예측\n",
    "    if ensemble_probs is not None:\n",
    "        ensemble_preds = torch.argmax(ensemble_probs, dim=1)\n",
    "        print(f\"앙상블 예측 완료: {len(ensemble_preds)}개 샘플\")\n",
    "    else:\n",
    "        print(\"앙상블 예측 실패: 예측할 모델이 없습니다.\")\n",
    "else:\n",
    "    print(\"val_f1_scores가 비어있습니다. 모델 학습을 먼저 완료해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./jar/val_f1_scores.pkl','wb') as f:\n",
    "    pickle.dump(val_f1_scores,f)\n",
    "with open('./jar/test_probs.pkl', 'wb') as f:\n",
    "    pickle.dump(test_probs,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 디코딩\n",
    "decoded_predictions_ensemble = [label_decoder[pred] for pred in ensemble_preds.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일 생성\n",
    "complete_time = time.strftime('%Y-%m-%d', time.localtime(time.time()))\n",
    "sample_submission = pd.read_csv(\"./data/sample_submission.csv\")  # 경로 수정\n",
    "sample_submission[\"분류\"] = decoded_predictions_ensemble\n",
    "sample_submission.to_csv(f\"./ensemble_submission_{complete_time}.csv\", encoding='UTF-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
